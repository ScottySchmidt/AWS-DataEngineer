{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b979132",
   "metadata": {
    "papermill": {
     "duration": 0.00192,
     "end_time": "2025-12-16T14:44:09.321197",
     "exception": false,
     "start_time": "2025-12-16T14:44:09.319277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AWS Data Engineer Pipeline\n",
    "The following data engineer system automatically ingests data, stores and processes it, and produces analytics outputs, all deployed through Infrastructure as Code (AWS CDK in Python). The architecture follows a four-part pattern: ingest → store → analyze → deploy. This project is adapted from a top industry data-engineering interview \"internship\" challenge. \n",
    "- Scott Schmidt — Illinois State University - Masters Computer Science for IT494 \n",
    "- Updated GitHub Version will be here:  https://github.com/ScottySchmidt/AWS-DataEngineer/tree/main\n",
    "\n",
    "## Project Summary\n",
    "This pipeline pulls data from public APIs, stores it in Amazon S3, processes it with AWS Lambda, and produces outputs suitable for analytics and reporting. All components, including compute, storage, permissions, and scheduling, are provisioned automatically using the AWS CDK in Python.\n",
    "\n",
    "---\n",
    "# Pipeline Architecture\n",
    "\n",
    "#### Part 1: AWS S3 & Sourcing Datasets\n",
    "1. Republish [this open dataset](https://download.bls.gov/pub/time.series/pr/) in Amazon S3 and share with us a link.\n",
    "    - You may run into 403 Forbidden errors as you test accessing this data. There is a way to comply with the BLS data access policies and re-gain access to fetch this data programatically. The BLS data access policies can be found here: https://www.bls.gov/bls/pss.htm . BLS also reserves the right to block robots that do not contain information that can be used to contact the owner. Blocking may occur in real time. An API key and  Adding a <code>User-Agent</code> header to your request with contact information will comply with the BLS data policies and allow you to keep accessing their data programmatically. \n",
    "2. Script this process so the files in the S3 bucket are kept in sync with the source when data on the website is updated, added, or deleted.\n",
    "    - Don't rely on hard coded names - the script should be able to handle added or removed files.\n",
    "    - Ensure the script doesn't upload the same file more than once.\n",
    "\n",
    "#### Part 2: APIs\n",
    "1. Create a script that will fetch data from [this API](https://honolulu-api.datausa.io/tesseract/data.jsonrecords?cube=acs_yg_total_population_1&drilldowns=Year%2CNation&locale=en&measures=Population).\n",
    "   You can read the documentation [here](https://datausa.io/about/api/).\n",
    "2. Save the result of this API call as a JSON file in S3.\n",
    "\n",
    "#### Part 3: Data Analytics\n",
    "0. Load both the csv file from **Part 1** `pr.data.0.Current` and the json file from **Part 2**\n",
    "   as dataframes ([Spark](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html),\n",
    "                  [Pyspark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html),\n",
    "                  [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html),\n",
    "                  [Koalas](https://koalas.readthedocs.io/en/latest/),\n",
    "                  etc).\n",
    "\n",
    "1. Using the dataframe from the population data API (Part 2),\n",
    "   generate the mean and the standard deviation of the annual US population across the years [2013, 2018] inclusive.\n",
    "\n",
    "2. Using the dataframe from the time-series (Part 1),\n",
    "   For every series_id, find the *best year*: the year with the max/largest sum of \"value\" for all quarters in that year. Generate a report with each series id, the best year for that series, and the summed value for that year.\n",
    "   For example, if the table had the following values:\n",
    "\n",
    "    | series_id   | year | period | value |\n",
    "    |-------------|------|--------|-------|\n",
    "    | PRS30006011 | 1995 | Q01    | 1     |\n",
    "    | PRS30006011 | 1995 | Q02    | 2     |\n",
    "    | PRS30006011 | 1996 | Q01    | 3     |\n",
    "    | PRS30006011 | 1996 | Q02    | 4     |\n",
    "    | PRS30006012 | 2000 | Q01    | 0     |\n",
    "    | PRS30006012 | 2000 | Q02    | 8     |\n",
    "    | PRS30006012 | 2001 | Q01    | 2     |\n",
    "    | PRS30006012 | 2001 | Q02    | 3     |\n",
    "\n",
    "    the report would generate the following table:\n",
    "\n",
    "    | series_id   | year | value |\n",
    "    |-------------|------|-------|\n",
    "    | PRS30006011 | 1996 | 7     |\n",
    "    | PRS30006012 | 2000 | 8     |\n",
    "\n",
    "#### Part 3: Data Analytics\n",
    "0. Load both the csv file from **Part 1** `pr.data.0.Current` and the json file from **Part 2**\n",
    "   as dataframes ([Spark](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html),\n",
    "                  [Pyspark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html),\n",
    "                  [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)\n",
    "\n",
    "1. Using the dataframe from the population data API (Part 2),\n",
    "   generate the mean and the standard deviation of the annual US population across the years [2013, 2018] inclusive.\n",
    "\n",
    "2. Using the dataframe from the time-series (Part 1),\n",
    "   For every series_id, find the *best year*: the year with the max/largest sum of \"value\" for all quarters in that year. Generate a report with each series id, the best year for that series, and the summed value for that year.\n",
    "   For example, if the table had the following values:\n",
    "\n",
    "    | series_id   | year | period | value |\n",
    "    |-------------|------|--------|-------|\n",
    "    | PRS30006011 | 1995 | Q01    | 1     |\n",
    "    | PRS30006011 | 1995 | Q02    | 2     |\n",
    "    | PRS30006011 | 1996 | Q01    | 3     |\n",
    "    | PRS30006011 | 1996 | Q02    | 4     |\n",
    "    | PRS30006012 | 2000 | Q01    | 0     |\n",
    "    | PRS30006012 | 2000 | Q02    | 8     |\n",
    "    | PRS30006012 | 2001 | Q01    | 2     |\n",
    "    | PRS30006012 | 2001 | Q02    | 3     |\n",
    "\n",
    "    the report would generate the following table:\n",
    "\n",
    "    | series_id   | year | value |\n",
    "    |-------------|------|-------|\n",
    "    | PRS30006011 | 1996 | 7     |\n",
    "    | PRS30006012 | 2000 | 8     |\n",
    "\n",
    "3. Using both dataframes from Part 1 and Part 2, generate a report that will provide the `value`\n",
    "   for `series_id = PRS30006032` and `period = Q01` and the `population` for that given year (if available in the population dataset).\n",
    "   The below table shows an example of one row that might appear in the resulting table:\n",
    "\n",
    "    | series_id   | year | period | value | Population |\n",
    "    |-------------|------|--------|-------|------------|\n",
    "    | PRS30006032 | 2018 | Q01    | 1.9   | 327167439  |\n",
    "\n",
    "    **Hints:** when working with public datasets you sometimes might have to perform some data cleaning first.\n",
    "   For example, you might find it useful to perform [trimming](https://stackoverflow.com/questions/35540974/remove-blank-space-from-data-frame-column-values-in-spark) of whitespaces before doing any filtering or joins\n",
    "\n",
    "\n",
    "4. Submit your analysis, your queries, and the outcome of the reports as a [.ipynb](https://fileinfo.com/extension/ipynb) file.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## How the Pipeline Works\n",
    "1. Local or Lambda-based ingestion fetches API data and uploads it to S3  \n",
    "2. Hash-based sync prevents uploading unchanged files  \n",
    "3. S3 events trigger processing Lambdas (optionally via SQS)  \n",
    "4. Pandas cleans and summarizes the data  \n",
    "5. Outputs are written back to S3 for reporting or dashboard use  \n",
    "6. CDK deploys all compute, storage, scheduling, and permissions  \n",
    "7. Video demonstration and user documentation (if applicable)\n",
    "8. Document use AI as a reference tool and exhibit full understanding.\n",
    "\n",
    "---- \n",
    "### AWS Infrastructure\n",
    "- Amazon S3 — Raw and processed data storage  \n",
    "- AWS Lambda — Ingestion and transformation compute  \n",
    "- Amazon SQS — Event queue for decoupled processing  \n",
    "- Amazon EventBridge — Scheduled API ingestion  \n",
    "- AWS CDK (Python) — Infrastructure as Code  \n",
    "- AWS IAM — Secure role-based permissions  \n",
    "\n",
    "### Technologies\n",
    "- Python using Pandas and Boto3  \n",
    "- AWS Secrets Manager, GitHub Secrets  \n",
    "- Public APIs: BLS, DataUSA\n",
    "  \n",
    "---\n",
    "#### Enhanced Ideas\n",
    "* **CI/CD Workflow (Optional)**  GitHub Actions workflow for automated CDK deployment on commit.\n",
    "* Use Amazon Redshift and Amazon Athena for SQL queries for the below:\n",
    "https://github.com/rearc-data/analytics-quest\n",
    "* Replace the processing Lambda with AWS Glue or AWS Step Functions for complex orchestration.\n",
    "* Introduce cost anomaly detection using AWS Budgets and SNS alerts.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.176881,
   "end_time": "2025-12-16T14:44:09.742831",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T14:44:03.565950",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
