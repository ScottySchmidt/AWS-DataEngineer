{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb382065",
   "metadata": {
    "papermill": {
     "duration": 0.001955,
     "end_time": "2025-12-13T16:10:50.396601",
     "exception": false,
     "start_time": "2025-12-13T16:10:50.394646",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AWS Data Engineer Pipeline\n",
    "The following project is based on a cloud data engineering internship-style challenge. This document intentionally omits any mention of external companies following academic guidelines. The system automatically ingests data, stores and processes it, and produces data analytics outputs, all deployed using Infrastructure as Code (AWS CDK in Python). The architecture follows a four-part pattern: ingest → store → analyze → deploy\n",
    "\n",
    "Scott Schmidt — Illinois State University — IT494 \n",
    "\n",
    "Updated GitHub Version:  https://github.com/ScottySchmidt/AWS-DataEngineer/tree/main\n",
    "\n",
    "---\n",
    "\n",
    "## Project Summary\n",
    "This pipeline pulls data from public APIs, stores it in Amazon S3, processes it with AWS Lambda, and produces outputs suitable for analytics and reporting. All components—including compute, storage, permissions, and scheduling—are provisioned automatically using the AWS CDK (Python).\n",
    "\n",
    "---\n",
    "\n",
    "## Data Pipeline Process\n",
    "\n",
    "1. **Jupyter Notebook — API Ingestion (Local Sync Version)**  \n",
    "   - Uses the **BLS public API** (standard datasets and bulk files)  \n",
    "   - Fetches BLS time-series data from  \n",
    "     `https://download.bls.gov/pub/time.series/pr/`  \n",
    "   - Uses compliant `User-Agent` headers per BLS data access guidelines  \n",
    "   - Hash-based change detection to prevent duplicate uploads  \n",
    "   - Cleans and formats JSON/CSV prior to upload  \n",
    "   - Enhanced sync mode automatically adds, updates, and deletes files in S3  \n",
    "\n",
    "2. **AWS Lambda — API → S3**  \n",
    "   Serverless function that retrieves data from public APIs and stores JSON files in Amazon S3.\n",
    "\n",
    "3. **Jupyter Notebook — Data Processing & Reporting**  \n",
    "   Loads raw S3 datasets, cleans them using Pandas, and generates summarized analytical outputs.\n",
    "\n",
    "4. **Infrastructure as Code — AWS CDK**  \n",
    "   CDK project that deploys:\n",
    "   - S3 buckets  \n",
    "   - Lambda functions  \n",
    "   - SQS queues  \n",
    "   - EventBridge schedules  \n",
    "   - IAM roles  \n",
    "\n",
    "5. **CI/CD Workflow (Optional)**  \n",
    "   GitHub Actions workflow for automated CDK deployment on commit.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "### Ingest\n",
    "- Lambda functions call public APIs such as:\n",
    "  - BLS Dataset (Producer Price Index):  \n",
    "    `https://download.bls.gov/pub/time.series/pr/`\n",
    "  - DataUSA API:  \n",
    "    `https://honolulu-api.datausa.io`\n",
    "- A local notebook can also ingest and sync large datasets prior to cloud processing.\n",
    "\n",
    "### Store\n",
    "- Amazon S3 stores raw JSON, CSV files, and processed outputs  \n",
    "- Data is organized by dataset type and timestamp using lakehouse-style conventions\n",
    "\n",
    "### Analyze\n",
    "- S3 events trigger processing Lambdas (optionally through SQS)  \n",
    "- Data is cleaned, aggregated, and written back to S3\n",
    "\n",
    "### Deploy\n",
    "- All infrastructure is created using AWS CDK (Python)  \n",
    "- Supports deployment from local environments, AWS CloudShell, or CI/CD pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment Options\n",
    "- Local Jupyter Notebook + Python CDK  \n",
    "- AWS CloudShell CDK deployment  \n",
    "- GitHub Actions CI/CD (optional)  \n",
    "\n",
    "---\n",
    "\n",
    "## AWS Services Used\n",
    "- Amazon S3 — Raw and processed data storage  \n",
    "- AWS Lambda — Ingestion and transformation compute  \n",
    "- Amazon SQS — Event queue for decoupled processing  \n",
    "- Amazon EventBridge — Scheduled API ingestion  \n",
    "- AWS CDK (Python) — Infrastructure as Code  \n",
    "- AWS IAM — Secure role-based permissions  \n",
    "\n",
    "---\n",
    "## Technologies\n",
    "- Python  \n",
    "- Pandas  \n",
    "- Boto3  \n",
    "- AWS Secrets Manager, GitHub Secrets  \n",
    "- Public APIs: BLS, DataUSA\n",
    "  \n",
    "## How the Pipeline Works\n",
    "1. Local or Lambda-based ingestion fetches API data and uploads it to S3  \n",
    "2. Hash-based sync prevents uploading unchanged files  \n",
    "3. S3 events trigger processing Lambdas (optionally via SQS)  \n",
    "4. Pandas cleans and summarizes the data  \n",
    "5. Outputs are written back to S3 for reporting or dashboard use  \n",
    "6. CDK deploys all compute, storage, scheduling, and permissions  \n",
    "7. Video demonstration and user documentation (if applicable)\n",
    "   \n",
    "---\n",
    "### Enchanced Ideas:\n",
    "Use Amazon Redshift and Amazon Athena for SQL queries for the below:\n",
    "* https://github.com/rearc-data/analytics-quest"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.663546,
   "end_time": "2025-12-13T16:10:50.817654",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-13T16:10:45.154108",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
